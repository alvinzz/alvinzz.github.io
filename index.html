<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Alvin Zhang</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Alvin Zhang</div>
<div class="menu-item"><a href="index.html" class="current">About</a></div>
<div class="menu-item"><a href="cv.pdf">CV</a></div>
<div class="menu-item"><a href="projects.html">Projects</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Alvin Zhang</h1>
</div>
<table class="imgtable"><tr><td>
<img src="files/profile.jpeg" alt="" width="250px" />&nbsp;</td>
<td align="left"><p>Research Engineer, <a href="http://matician.com">Matician</a></p>
<p>Github: <a href="https://github.com/alvinzz">https://github.com/alvinzz</a></p>
<p>Email: alvn.zng [at] gmail</p>
</td></tr></table>
<h2>About</h2>
<p>I am a currently a research engineer at <a href="http://matician.com">Matician</a>, working on perception for autonomous robots.</p>
<p>Over the past summer, I worked with <a href="https://redwood.berkeley.edu/people/bruno-olshausen/">Dr. Bruno Olshausen</a> in the <a href="https://redwood.berkeley.edu">Redwood Center for Theoretical Neuroscience</a> at UC Berkeley.</p>
<p>Previously, I recieved a B.Sc. in Electrical Engineering and Computer Science from UC Berkeley.</p>
<h2>Goals</h2>
<p>I am seeking a Ph.D. in Computer Vision or Robotics.</p>
<h2>Research Interests</h2>
<h3>Perception</h3>
<p>Self-supervision through self-consistency is a rich signal for <b>training</b> neural networks.
Can these objectives also be optimized <b>during test-time inference</b>?</p>
<div class="wrap-collabsible">
  <input id="collapsible" class="toggle" type="checkbox">
  <label for="collapsible" class="lbl-toggle">Why?</label>
  <div class="collapsible-content">
    <div class="content-inner">
      <p>
        A system that <b>incorporates sensory feedback to solve an optimization problem at run-time</b> has three key properties:
      </p>  
      <ol>
        <li>The system self-corrects its behavior or internal state based on external observations.</li>
        <li>The system has a metric for performance, provided by the objective function.</li>
        <li>The system's performance improves with additional compute.</li>
      </ol>
      <p>
        Property <b>2</b> has important implications for robustness and safety:
        if a system's internal state estimate disagrees with observations,
        or the cost of a control trajectory is excessively high, the system can
        stop itself and request human intervention.
      </p>
      <p>
        Turning neural networks into closed-loop systems may help them to match the reliability of classical closed-loop control,
        which remains the <i>de facto</i> standard in industry despite recent advances in deep learning.
      </p>
    </div>
  </div>
</div>
<div class="wrap-collabsible">
  <input id="collapsible2" class="toggle" type="checkbox">
  <label for="collapsible2" class="lbl-toggle">How?</label>
  <div class="collapsible-content">
    <div class="content-inner">
      <p>
        <b>RAFT</b> is an iterative learned method for estimating optical flow.
        During test-time, inference is done by recursively applying a learned update operator to the flow estimate.
      </p>
      <p>
        However, it cannot incorporate feedback: if a red pixel is matched to a green one at the end of inference,
        the network can only return that incorrect result.
      </p>
      <p>
        I propose to directly optimize the flow estimate on a unsupervised objective (such as photometric error) during the inference
        process. This may be alternated with the learned update step or used on-line to tune the weights of the network for a particular video.
      </p>
      <br>
      <p>
        <b>NeRF</b> trains a neural network to implicitly represent a 3-D scene, so that volumetric renderings of the scene are consistent with observations (posed camera images).
      </p>  
      <p>
        In this sense, NeRF is robust because it knows if its scene representation is poor. However, learning does not transfer across scenes, which makes inference slow.
      </p>
      <p>
        This suggests that priors can be used to speed up inference. This would also improve 3-D modelling in the sparse-view case, or in scenarios with noisy poses.
      </p>
      <p>
        These priors could come in the form of
        <ol>
          <li>Known geometric priors, such as edge sparsity. (Related work: Plenoxels, Compressed Sensing MRI, NeRS, RegNeRF).</li>
          <li>Learned image-based priors, such as estimated depth. (Related work: PixNeRF, MonoSDF, RegNeRF).</li>
          <li>Learned 3-D priors via generative models. (Related work: AutoSDF, GIRAFFE).</li>
        </ol>
        For this <b>3</b>rd category of priors, I propose exploring conditional diffusion networks as generative models.
      </p>
    </div>
  </div>
</div>
<h3>Robotics</h3>
<p>Can we use <b>touch and proprioception</b> to develop <b>actionable and observationally consistent representations</b> for robotic behavior?</p>
<div class="wrap-collabsible">
  <input id="collapsible3" class="toggle" type="checkbox">
  <label for="collapsible3" class="lbl-toggle">Why?</label>
  <div class="collapsible-content">
    <div class="content-inner">
      <p>
        Robots are different from computers because they <b>interact with the physical world</b>.
      </p>
      <p>
        Yet, touch, our fundamental sense for interacting with the world, has been overlooked in robotics research.
      </p>
      <p>
        A robot needs to ground its visual observations via touch and proprioception to ensure that its
        mental model of the world remains consistent with reality.
      </p>
      <p>
        If there is a mismatch between visual and haptic feedback,
        the robot can then adjust its world model and its actions to reflect greater uncertainty, exercise more caution, and seek new information.
      </p>
      <p>
        Over the long term, it can also use this difference to update its learned perception module in a self-supervised manner.
        This will then allow the robot to exhibit more robust and adaptive behavior in novel environments.
      </p>
    </div>
  </div>
</div>
<div class="wrap-collabsible">
  <input id="collapsible4" class="toggle" type="checkbox">
  <label for="collapsible4" class="lbl-toggle">How?</label>
  <div class="collapsible-content">
    <div class="content-inner">
      <p>
        <b>NeRF</b> introduced a neural representation which enforces multi-view <b>visual</b> consistency. However, as mentioned above, a system that interacts with the real world also needs to incorporate touch.
      </p>
      <p>
        I propose to extend this framework to additionally <b>enforce haptic and proprioceptive constraints</b>. This should also be used to <b>guide behavior</b> if the state estimate has high error.
        (Related work: Cross-Modal Supervision, Perception-aware Path Planning).
      </p>
      <br>
      <p>
        In addition, NeRF is optimized for photometric consistency. Such pixel-level detail is rarely needed for robotic tasks, making NeRF too expensive to run on-device in a real-time system.
      </p>
      <p>
        Instead, I propose to distill NeRF's volumetric representation into a task-specific, actionable representation. (Related work: Denoised MDPs, Curiousity-Driven Exploration by Self-supervised Prediction, "Visual attention prediction improves performance of
autonomous drone racing agents").
      </p>
    </div>
  </div>
</div>
<h2>Projects</h2>
<p>See my <a href="projects.html">Projects</a> page.</p>
<h2>Publications</h2>
<p>A. Zhang, &ldquo;Generalized Skill Learning, Safety, and Exploration with Flow-Based Models&rdquo;, Workshop on Task-Agnostic Reinforcement Learning, International Conference on Learning Representations, 2019.</p>
<div id="footer">
<div id="footer-text">
Page generated 2022-12-17 09:16:25 PST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
(<a href="index.jemdoc">source</a>)
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
